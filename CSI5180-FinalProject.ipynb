{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S1sYw_LdrGx"
      },
      "source": [
        "This notebook is an implementation by Mostafa El Katerji and is an experiment for leveraging Transfer Learning in the domain of intent detection. It was implemented for the CSI5180 course on Virtual Assistants at uOttawa. It builds on previous work referenced below.\n",
        "\n",
        "References:\n",
        "- Multi-Domain Joint Semantic Frame Parsing using Bi-directional RNN-LSTM\n",
        "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/IS16_MultiJoint.pdf \n",
        "- ATIS Dataset\n",
        "https://www.kaggle.com/hassanamin/atis-airlinetravelinformationsystem \n",
        "- An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction\n",
        "https://github.com/clinc/oos-eval \n",
        "- A Deep Multi-task Model for Dialogue Act Classification, Intent Detection and Slot Filling\n",
        "https://link.springer.com/article/10.1007/s12559-020-09718-4 \n",
        "- A survey of joint intent detection and slot-filling models in natural language understanding\n",
        "https://arxiv.org/pdf/2101.08091.pdf \n",
        "- tf.keras.preprocessing.text.Tokenizer\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer \n",
        "- sklearn.preprocessing.LabelEncoder\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html \n",
        "- Chatbots: Intent Recognition Dataset\n",
        "https://www.kaggle.com/elvinagammed/chatbots-intent-recognition-dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT4DXCAbFhwV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eo6HAqrbx7w"
      },
      "source": [
        "!mkdir results\n",
        "!mkdir results/OOS\n",
        "!mkdir results/Atis\n",
        "!mkdir results/CB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC6xxhiaFXqg"
      },
      "source": [
        "import json\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.layers as layers\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "nltk.download('stopwords')\n",
        "all_english_stop_words = stopwords.words('english')\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# The path to a folder that contains the dataset files with names \"atis_intents.csv\", \"chatbots-intent-recognition-dataset.json\", and \"oos-eval-full.json\"\n",
        "# https://www.kaggle.com/hassanamin/atis-airlinetravelinformationsystem/version/1\n",
        "# https://www.kaggle.com/elvinagammed/chatbots-intent-recognition-dataset\n",
        "# https://github.com/clinc/oos-eval\n",
        "VA_PROJECT_DATA =  \"./datasets\"\n",
        "\n",
        "# The path to a folder where the results will be saved. It must contain three folders named \"OOS\", \"Atis\", and \"CB\"\n",
        "VA_PROJECT_RES = \"./results\"\n",
        "\n",
        "# Parse the datasets\n",
        "class Dataset:\n",
        "    def __init__(self, intent_texts, intent_labels):\n",
        "        self.intent_texts = intent_texts\n",
        "        self.intent_labels = intent_labels\n",
        "\n",
        "def parse_oos_dataset():\n",
        "    with open(f'{VA_PROJECT_DATA}/oos-eval-full.json') as f:\n",
        "        ds = json.load(f)\n",
        "\n",
        "    intents = []\n",
        "    intent_labels = []\n",
        "\n",
        "    for key in ds.keys():\n",
        "        for elem in ds[key]:\n",
        "            intent = elem[0]\n",
        "            intent_label = elem[1]\n",
        "\n",
        "            if 'oos' not in intent_label:\n",
        "                intents.append(intent)\n",
        "                intent_labels.append(intent_label)\n",
        "\n",
        "    return Dataset(intents, intent_labels)\n",
        "\n",
        "def parse_atis_dataset():\n",
        "    with open(f'{VA_PROJECT_DATA}/atis_intents.csv') as f:\n",
        "        lines = f.read().split('\\n')\n",
        "       \n",
        "    intents = []\n",
        "    intent_labels = []\n",
        "\n",
        "    for line in lines:\n",
        "        pair = line.split(\",\")\n",
        "        if(len(pair) > 1):\n",
        "            intents.append(pair[1].strip())\n",
        "            intent_labels.append(pair[0].strip())\n",
        "    \n",
        "    return Dataset(intents, intent_labels)\n",
        "\n",
        "def parse_cb_dataset():\n",
        "    with open(f'{VA_PROJECT_DATA}/chatbots-intent-recognition-dataset.json') as f:\n",
        "        ds = json.load(f)\n",
        "\n",
        "    intents = []\n",
        "    intent_labels = []\n",
        "\n",
        "    for elem in ds['intents']:\n",
        "        intent = elem['intent']\n",
        "        for text in elem['text']:\n",
        "            intents.append(text)\n",
        "            intent_labels.append(intent)\n",
        "\n",
        "    return Dataset(intents, intent_labels)\n",
        "\n",
        "# clean the text\n",
        "def clean_dataset_text(ds):\n",
        "    for i in range(0, len(ds.intent_texts)):\n",
        "        # remove punctiation\n",
        "        curr_text = re.sub(r'[^\\w\\s]', '', ds.intent_texts[i])\n",
        "\n",
        "        # remove stop words and stem the text\n",
        "        output_text = \"\"\n",
        "        for word in curr_text.split(\" \"):\n",
        "            if word not in all_english_stop_words:\n",
        "                output_text += \" \" + porter_stemmer.stem(word)\n",
        "        ds.intent_texts[i] = output_text.strip()\n",
        "\n",
        "# tokenize the dataset\n",
        "def create_tokenizer(texts):\n",
        "    # init the tokenizer\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
        "    # fit the tokenizer on all the texts\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def tokenize(tokenizer, texts):\n",
        "    # generate the input sequences\n",
        "    input_sequences = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(texts), padding='pre')\n",
        "    return input_sequences\n",
        "\n",
        "# categorize labels\n",
        "def create_category_encoder(labels):\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(labels)\n",
        "\n",
        "    return label_encoder\n",
        "\n",
        "def categorize_labels(label_encoder, labels):\n",
        "    vec = label_encoder.transform(labels)\n",
        "    return tf.keras.utils.to_categorical(vec, num_classes=194)\n",
        "\n",
        "# build the models\n",
        "def build_baseline_model(word_count, number_of_intents, freeze_top = False, weights = None):\n",
        "    gru_layer = layers.GRU(4)\n",
        "    model = models.Sequential([\n",
        "        layers.Embedding(word_count, 356),\n",
        "        layers.Bidirectional(gru_layer),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(number_of_intents, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    if weights is not None:\n",
        "      model.set_weights(weights)\n",
        "\n",
        "    if freeze_top:\n",
        "      for li in range(0, len(model.layers) - 1):\n",
        "        model.layers[li].trainable = False\n",
        "      gru_layer.trainable = False\n",
        "      model.pop()\n",
        "      model.add(layers.Dense(number_of_intents, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_final_model(word_count, number_of_intents, freeze_top = False, weights = None):\n",
        "    model = models.Sequential([\n",
        "        layers.Embedding(word_count, 356),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv1D(128, 5, activation='relu'),\n",
        "        layers.GlobalMaxPooling1D(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(number_of_intents, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    if weights is not None:\n",
        "      model.set_weights(weights)\n",
        "\n",
        "    if freeze_top:\n",
        "      for li in range(0, len(model.layers) - 1):\n",
        "        model.layers[li].trainable = False\n",
        "      model.pop()\n",
        "      model.add(layers.Dense(number_of_intents, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# fit the model\n",
        "def fit_model(name, sub_folder, model, input_sequences, categorized_labels, epochs, batch_size):\n",
        "    print(name)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(input_sequences, categorized_labels, test_size=0.2)\n",
        "  \n",
        "    callback = EarlyStopping(monitor='val_accuracy', patience=300, restore_best_weights = True)\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, batch_size=batch_size, callbacks=[callback])\n",
        "\n",
        "    training_metrics = {\n",
        "        \"accuracy\": history.history['accuracy'],\n",
        "        \"val_accuracy\": history.history['val_accuracy']\n",
        "    }\n",
        "\n",
        "    results = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "    print(\"val loss, val acc:\", results)\n",
        "\n",
        "    with open('{}/{}/{}-history.json'.format(VA_PROJECT_RES, sub_folder, name), 'w', encoding='utf-8') as f:\n",
        "        json.dump(training_metrics, f, ensure_ascii=False, indent=4)\n",
        "    with open('{}/{}/{}-evaluate.json'.format(VA_PROJECT_RES, sub_folder, name), 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title(f'{name} train:{round(history.history[\"accuracy\"][-1] * 100, 2)} val:{round(history.history[\"val_accuracy\"][-1] * 100, 2)} test:{round(results[1] * 100, 2)}'[3:])\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='lower left')\n",
        "    plt.ylim(0,1)\n",
        "    plt.savefig('{}/{}/{}-acc.png'.format(VA_PROJECT_RES, sub_folder, name))\n",
        "    plt.clf()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx0RuD8Q6QxF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5cSeFZnG_PJ"
      },
      "source": [
        "# parse the datasets\n",
        "oos_dataset = parse_oos_dataset()\n",
        "cb_dataset = parse_cb_dataset()\n",
        "atis_dataset = parse_atis_dataset()\n",
        "\n",
        "# clean the datasets\n",
        "clean_dataset_text(oos_dataset)\n",
        "clean_dataset_text(cb_dataset)\n",
        "clean_dataset_text(atis_dataset)\n",
        "\n",
        "# create a common tokenizer and category encoder\n",
        "combined_texts = oos_dataset.intent_texts + cb_dataset.intent_texts + atis_dataset.intent_texts\n",
        "tokenizer = create_tokenizer(combined_texts)\n",
        "combined_labels_comb = oos_dataset.intent_labels + cb_dataset.intent_labels + atis_dataset.intent_labels\n",
        "category_encoder = create_category_encoder(list(set(combined_labels_comb)))\n",
        "\n",
        "# tokenize the text from oos\n",
        "input_sequences_oos = tokenize(tokenizer, oos_dataset.intent_texts)\n",
        "\n",
        "# tokenize the text from cb\n",
        "input_sequences_cb = tokenize(tokenizer, cb_dataset.intent_texts)\n",
        "\n",
        "# tokenize the text from atis\n",
        "input_sequences_atis = tokenize(tokenizer, atis_dataset.intent_texts)\n",
        "\n",
        "# tokenize the text from both datasets\n",
        "input_sequences_comb = tokenize(tokenizer, combined_texts)\n",
        "print(len(list(set(combined_labels_comb))))\n",
        "# categorize labels from oos\n",
        "categorized_labels_oos = categorize_labels(category_encoder, oos_dataset.intent_labels)\n",
        "\n",
        "# categorize labels from cb\n",
        "categorized_labels_cb = categorize_labels(category_encoder, cb_dataset.intent_labels)\n",
        "\n",
        "# categorize labels from atis\n",
        "categorized_labels_atis = categorize_labels(category_encoder, atis_dataset.intent_labels)\n",
        "\n",
        "# categorize labels from both datasets\n",
        "categorized_labels_comb = categorize_labels(category_encoder, combined_labels_comb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHrtiLcqHKN1"
      },
      "source": [
        "plot_model(build_baseline_model(len(tokenizer.word_index) + 1, categorized_labels_oos.shape[1]), to_file='baseline.png', show_shapes=True, show_layer_names=True)\n",
        "plot_model(build_final_model(len(tokenizer.word_index) + 1, categorized_labels_oos.shape[1]), to_file='final.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MivycjoHHDeT"
      },
      "source": [
        "# build the models\n",
        "nbr_of_words = len(tokenizer.word_index) + 1\n",
        "nbr_of_cat = categorized_labels_comb.shape[1]\n",
        "\n",
        "dataset_config = {\n",
        "    \"OOS\": {\n",
        "        \"is\": input_sequences_oos,\n",
        "        \"cl\": categorized_labels_oos,\n",
        "        \"modelBl\": build_baseline_model(nbr_of_words, nbr_of_cat),\n",
        "        \"modelFl\": build_final_model(nbr_of_words, nbr_of_cat),\n",
        "        \"epochs\": 60,\n",
        "        \"batchSize\": 128,\n",
        "    },\n",
        "    \"CB\": {\n",
        "        \"is\": input_sequences_cb,\n",
        "        \"cl\": categorized_labels_cb,\n",
        "        \"modelBl\": build_baseline_model(nbr_of_words, nbr_of_cat),\n",
        "        \"modelFl\": build_final_model(nbr_of_words, nbr_of_cat),\n",
        "        \"epochs\": 250,\n",
        "        \"batchSize\": 16,\n",
        "    },\n",
        "    \"Atis\": {\n",
        "        \"is\": input_sequences_atis,\n",
        "        \"cl\": categorized_labels_atis,\n",
        "        \"modelBl\": build_baseline_model(nbr_of_words, nbr_of_cat),\n",
        "        \"modelFl\": build_final_model(nbr_of_words, nbr_of_cat),\n",
        "        \"epochs\": 60,\n",
        "        \"batchSize\": 128,\n",
        "    }\n",
        "}\n",
        "\n",
        "for el in dataset_config:\n",
        "  model_bl = dataset_config[el][\"modelBl\"]\n",
        "  model_fnl = dataset_config[el][\"modelFl\"]\n",
        "  inp_seq = dataset_config[el][\"is\"]\n",
        "  cl = dataset_config[el][\"cl\"]\n",
        "  epochs = dataset_config[el][\"epochs\"]\n",
        "  batch_size = dataset_config[el][\"batchSize\"]\n",
        "\n",
        "  model_bl.summary()\n",
        "  model_fnl.summary()\n",
        "\n",
        "  fit_model(f\"1. {el} from scratch baseline\", el, model_bl, inp_seq, cl, epochs, batch_size)\n",
        "  fit_model(f\"A. {el} from scratch final\", el, model_fnl, inp_seq, cl, epochs, batch_size)\n",
        "\n",
        "  old_weights_bl = model_bl.get_weights()\n",
        "  old_weights_fnl = model_fnl.get_weights()\n",
        "\n",
        "  tl_datasets = list(dataset_config.keys())\n",
        "  tl_datasets.remove(el)\n",
        "\n",
        "  for other_ds in tl_datasets:\n",
        "    ods_is = dataset_config[other_ds][\"is\"]\n",
        "    ods_cl = dataset_config[other_ds][\"cl\"]\n",
        "    ods_epochs = dataset_config[other_ds][\"epochs\"]\n",
        "    ods_batch_size = dataset_config[other_ds][\"batchSize\"]\n",
        "\n",
        "    # Transfer learning on frozen top weights\n",
        "    ods_model_bl = build_baseline_model(nbr_of_words, nbr_of_cat, True, old_weights_bl)\n",
        "    ods_model_fnl = build_final_model(nbr_of_words, nbr_of_cat, True, old_weights_fnl)\n",
        "\n",
        "    fit_model(f\"2. {other_ds} using {el} weights baseline\", other_ds, ods_model_bl, ods_is, ods_cl, ods_epochs, ods_batch_size)\n",
        "    fit_model(f\"B. {other_ds} using {el} weights final\", other_ds, ods_model_fnl, ods_is, ods_cl, ods_epochs, ods_batch_size)\n",
        "\n",
        "    # Transfer learning unfreze\n",
        "    ods_model_bl = build_baseline_model(nbr_of_words, nbr_of_cat, False, ods_model_bl.get_weights())\n",
        "    ods_model_fnl = build_final_model(nbr_of_words, nbr_of_cat, False, ods_model_fnl.get_weights())\n",
        "\n",
        "    fit_model(f\"3. {other_ds} using {el} weights baseline\", other_ds, ods_model_bl, ods_is, ods_cl, ods_epochs, ods_batch_size)\n",
        "    fit_model(f\"C. {other_ds} using {el} weights final\", other_ds, ods_model_fnl, ods_is, ods_cl, ods_epochs, ods_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}